---
title: "Week 3"
author: "David Kane"
output: html_document
---

Go to https://registrar.fas.harvard.edu/faculty-staff/courses/enrollment and scroll down to "Access the Current Course Enrollment Numbers." Click on that to download the Excel file. Create a folder in your project called `new_data`. Move the Excel file into that folder. Note that, even if you did this last week, you are doing it again because Harvard has updated the file. The file might be dated either September 21 or 22. We won't know till class!

Note that I have already created a directory called "old_data" and included the file from September 1 in it. Because I am your buddy, I even gove you the code for reading it in! (Although I did leave one mistake for you to find . . .)

Load **tidyverse**, **readxl* and **janitor** into your setup chunk. You may need to install the packages **readxl** and **janitor** if you haven't already.

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(janitor)
knitr::opts_chunk$set(echo = TRUE)

# Most groups got something like this going last week. Note the use of skip = 3
# to get rid of the garbage rows at the top of the file. Note the is.na()
# filter, which gets rid of the rows at the bottom, especially the dangerous
# summary row. Raw excel sheets are dangerous! Note that it was easy to naively
# assume that there was only one row per class. Untrue!

sep_old <- 
  read_excel("old_data/class_enrollment_summary_by_term_9-1-2020.xlsx", 
             skip = 3) %>% 
  clean_names() %>% 
  filter(! is.na(course_title)) %>% 
  select(-grad, -non_degree, -x_reg, -vus, -employee, 
         -withdraw, -total, -instructor_full_name, -course_section_code) %>%
  rename(id = course_id,
         title = course_title,
         name = course_name,
         department = course_department) %>% 
  group_by(id, title, name, department) %>% 
  summarize(u_grad = sum(u_grad), .groups = "drop") %>% 
  filter(u_grad > 10)

# But this is not correct! Look for this course. What do you see? What can you
# do to clean it up?

```

### Scene 1

**Prompt:** First, figure out what is wrong with sep_1 object. Edit the above code to fix it.

Second, read in and clean the new data, creating an object called either `sep_21` or `sep_22` depending on its date. 


**Answer:**

```{r s1}
# I selected out every variable I wanted to get rid of individually, but you can
# do this in a faster way using something like
# select(course_id:instructor_full_name). You need to skip because of garbage in
# the first few rows. You need to filter the rows without a course_title because
# of the summary row at the bottom. You need to group_by() because there are
# multiple rows for the same course. Change course_department to a factor
# variable to make it easier to work with later on.

sep_new <- 
  read_excel("raw_data/class_enrollment_summary_by_term_9-16-2020.xlsx", 
             skip = 3) %>% 
  clean_names() %>% 
  filter(! is.na(course_title)) %>% 
  select(-grad, -non_degree, -x_reg, -vus, -employee, 
         -withdraw, -total, -instructor_full_name, -course_section_code) %>%
  rename(id = course_id,
         title = course_title,
         name = course_name,
         department = course_department) %>%   
  group_by(id, title, name, department) %>% 
  summarize(u_grad = sum(u_grad), .groups = "drop") %>% 
  filter(u_grad > 10) 



```

**Comments:**


### Scene 2

Dean Amanda Claybaugh is concerned about the drop in undergradaute enrollment in some courses between September 1 and today. She wants you to analyze this issue. Before you dive into the details provide some bullet points as to how Wisdom and Temperance apply to this situation:

Before we start on the model, Wisdom suggests we should:


After we have a model, Temperance suggests we should:


**Comments:** Obviously, there are no right answers here. But we want to start to hit the key themes. Read the Cardinal Virtues Wikpedia page: https://en.wikipedia.org/wiki/Cardinal_virtues. Wisdom would ask us: Why are we even exploring this issue? Is enrollment a good guide to action? Should we even care which courses have increases in decreases? Are such changes correlated with anything? Are they biased? 

Temperance suggests that, even if we come up with a "model," we should be very careful of trying to use it in the future. Things change, Unknown/unknowns. Will whatever "causes" we discover today still have the same effects post-pandemic?


### Scene 3

**Prompt:** Which classes had the biggest increases and decreases in undergraduate enrollments between September 1 and today?


**Answer:**
```{r s3}
# you should create a new object in which to store your merged datasets.
# Left_join() will be the appropriate join to use here, and use the suffix
# argument to clearly indicate which columns are from which dataset. since we
# care about finding the change in enrollment from 2019 to 2020, the only
# variable we care about from 2019 is the undergraduate enrollment. you will
# need to drop_na() or you may run into an error with having NA values in the
# merged dataset. finally, create a new variable that shows the change in 
# enrollment.

enrollment_change <- left_join(sep_old, sep_new, 
                               by = "id", 
                               suffix = c("_old", "_new")) %>% 
  select(-title_old, -name_old, -department_old) %>% 
  drop_na() %>% 
  mutate(change = u_grad_old - u_grad_new) %>% 
  arrange(desc(change))

# here I have created two separate tibbles to find the top 5 courses with the
# greatest enrollment increase and the top 5 courses with the greatest
# enrollment decrease.

top_inc <- enrollment_change %>% 
  arrange(desc(en_change)) %>% 
  slice_head(n = 5)

top_dec <- enrollment_change %>% 
  arrange(en_change) %>% 
  slice_head(n = 5) 

# then, I join together those tibbles using full_join which simply puts all rows
# from both datasets into one. however, you will have to reorder your courses
# again when doing ggplot using reorder()

joined <- full_join(top_inc, top_dec) %>% 
  ggplot(mapping = aes(x = reorder(course_name.2020, en_change), y = en_change)) +
  geom_col() +
  coord_flip() +
  labs(y = "Change in Enrollment",
       title = "Courses with Greatest Increase and Decrease in Enrollment") +
  theme(axis.title.y = element_blank())

joined
```

**Comments:**


## Scene 4

What might have caused drops in these classes? Assume that one of the causes might have been the amount of work assigned in the first two weeks of class. Describe an ideal Preceptor Table with no missing data which would allow us to investigate this situation. What data is missing and why? With our actual Preceptor Table, how might we investigate the effect of work assigned in the first two weeks? Would the estimated Average Treatment Effect be accurate? Why or why not?


